title: Automated Fantasy Baseball Strategy using Web Scraping, Pandas, and Bokeh
---
author: Zach Morrissey
---
body:

## Winning Fantasy Baseball in Style

Every year I've missed the title in fantasy baseball. This year, I'm attempting to change all of that by using a number of automated inputs that will alert me to any possible deals on players that I can get and how to do it. Every year our group normally comes up with some very good scouting information, and this year I'm attempting to beat them all out by automating it.

### The Issues With Pre-Season Scouting

The advantages of scouting out fantasy baseball players in our league evaporates about 3 rounds into the draft. Our group traditionally over-indexes on the drafting portion of it, and then adapts a flying-by-the-seat-of-your-pants approach to the rest of the season. I tend to make bad add/drop decisions as the season goes along. The goal of this project is to fix this particular shortcoming by providing myself with a methodology for doing this throughout the year.

## Getting Data

The data for this exercise comes from a number of places. For a good system, we'll need to retrieve roster data to know who is available, projections to see who is slated to do well, and mid-season player statistics to so who's currently doing well.

### ESPN Roster Data

Our team uses ESPN for our league, which happens to allow access to webpages so long as you take the time to [make the league public](http://games.espn.com/flb/resources/help/content?name=create-league-standard). Getting roster data is fairly straightforward; all you need to do is scrape the rosters page for that league ID. This is an example of that:

```python
import requests
from bs4 import BeautifulSoup

# rip your league id from your espn url
league_id = 1234567890
requests.get('http://games.espn.com/flb/leaguerosters?leagueId={}'.format(league_id))
```

### Projections

Player projections are common source of scouting information for our league. Since we do not use the standard scoring rules, the projections from ESPN are mostly worthless to us (not that they're of much value otherwise). For my own purposes, I like the [Depth Charts projection system](https://www.fangraphs.com/projections.aspx?pos=all&stats=bat&type=fangraphsdc) that they use, which is a combination of two other common projection systems (ZiPS and Steamer) weighted by an in-house projection for playing time.

For the implementation portion of this piece, I created a python script that downloads the information from Fangraphs and saves it as a csv file. For projections, this turned out to be a little trickier than I had intended it to be, since it was hooked up as a javascript postback using ASP.net forms and I had no experience with any of that.

#### ASP Forms Galore

After gleaning a bit of help from [this 2011 web scraping post I found helpful](https://scraperwiki.com/2011/11/how-to-get-along-with-an-asp-webpage/):
* This page uses the javascript `__doPostBack` function to invoke a post to the form that's on this page.
* Python's `requests` library _should_ be able to send the same parameters to download the file.
* Ripping the `__EVENTTARGET` and `__EVENTVALUE` values should be enough to get me going.

I was looking to try and capture this information by editing the site's JS to see if I could spit out those values, but it turns out they weren't really sending any values over the wire anyways. It was just one big ol' POST to the same page with no meaningful parameters.

#### Going POSTal

After some failed efforts to try and see if there was a way I could use the ASP.net forms built in functions to get the data nice and easily, it turns out that they weren't really even using the functionality in the first place. There were very few URL or form parameters on the request that I could specify, and the ones that I could didn't really even make sense to me.

When snooping through this stuff, I noticed that file downloads don't show up in chrome's network requests tab, which is frustrating. Instead I navigated over to the much-lower-level chrome network internals page (`chrome://net-internals/#events`) where you can see this stuff happen in real time. Be warned: You will see every part of every network request, and it adds up to a _lot_ of information being spat out to the screen at once. What this let me know is that I was hoping there was somewhere I could make a simple POST request to and get a response, but that didn't seem like it was going to work, so I needed to get more browser-y.

Enter web automation framework [selenium](https://stackoverflow.com/questions/12812001/how-to-download-a-file-from-a-asp-website-in-python) which was recommended for similar tasks. That effort produced this, which works for the CSV download:

```python
for date in date_range:
    scrape_data(date)
```

### In-Season Actuals

In addition to projections, having in-season results helps improve our judgment and add context to the projections. There are players whom are specifically favored by the projection systems (namely, players with long histories), and players who are treated rather unkindly (small sample sizes).

Getting the ongoing statistics on a per-player basis turned out to be much easier than anything related to the projections. With a little manipulation of URL parameters, you can easily create a page for scraping that contains all of the necessary information in one go.

* [Pitching Stats](https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=al&qual=20&type=c,36,37,38,40,-1,120,121,217,-1,24,41,42,43,44,-1,117,118,119,-1,6,45,124,-1,62,122,13&season=2017&month=0&season1=2017&ind=0&team=0&rost=0&age=0&filter=&players=0&page=1_100000)
* [Batting Stats](https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=al&qual=0&type=8&season=2017&month=0&season1=2017&ind=0&team=0&rost=0&age=0&filter=&players=0&page=1_10000)

Creating a short script to parse out player rows and add them to another csv for consumption later on turned out to be short work once I figured this bit out.

## Creating a Repeatable Analysis Using Pandas

[Pandas](https://pandas.pydata.org/) is very capable of producing these sorts of information on tabular data in a straightforward, repeatable manner. 
---
pub_date: 2018-03-11
---
twitter_handle: _zmsy
