title: My Very Own Airflow Cluster
---
author: Zach Morrissey
---
body:

As someone who writes a lot of one-off batch scripts, the rise of DAG (Directed Acyclic Graph) task engines is great. I've used [Luigi](https://github.com/spotify/luigi) at work for about 2 years now. For feeding the backend of this website with data, I decided to set up an Airflow cluster.

![Airflow Screenshot](airflow.png)

As opposed to this paradigm:

- Write a script.
- Set it up in `cron`.
- Check the logs whenever something goes wrong.

You get a whole lot more bang for your buck, with similar amounts of work. DAG paradigm is more like:

- Write a script.
- Write an airflow DAG, where you call that script.
- Set a schedule for the script.
- Check on it from the built-in Web UI.
- Stop and restart tasks whenever you want to.
- View the console output from the Web UI.

It _really_ helps certain types of batch processes scale past a certain point, while simplifying the process of managing and deploying them. You can relatively easily set up Airflow to schedule thousands of tasks without all that much more configuration past what you'd normally write.

## Words of Warning

There's a number of gotchas you should know prior to using one of the DAG engines, as they all fall into the same traps.

- `cron` is _incredibly_ difficult to beat for reliability. There's a reason it's ubiquitous. DAG engines are usually for tasks that need to scale past what `cron` is built for. If you're just trying to run a single script on the regular, stick with that instead.
- Usually there's a non-trivial amount of setup involved at the beginning.
- Sometimes scaling can be a pain. For Airflow, you'll need to setup a task queue like Celery and a number of different nodes (web server, database, etc) once you scale past a certain point.
- If you're on Google Cloud Platform, they offer a hosted Airflow service called [Cloud Composer](https://cloud.google.com/composer/) which could be worth it if you want to avoid the difficulties in setting up and deploying Airflow.
- There's a [relatively mature Docker image](https://github.com/puckel/docker-airflow) that you can reuse if you don't want to configure it yourself.

## Diving In

I set up two nodes for this in my [Proxmox cluster](https://www.proxmox.com/en/):

1. **Airflow web server.** This was just based on a Ubuntu 16.04 instance.
2. **Postgres backend.** I opted to use the local scheduler, which can use Postgres as a backend, and not set up some of the larger scale options like Celery.

I'm using Postgres as a backend so that I can use the local executor, as opposed to the built-in sequential executor.


---
pub_date: 2018-06-25
---
slug_title: airflow
---
twitter_handle: _zmsy
