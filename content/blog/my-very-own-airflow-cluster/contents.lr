title: My Very Own Airflow Cluster
---
author: Zach Morrissey
---
body:

As someone who writes a lot of one-off batch scripts, the rise of DAG (Directed Acyclic Graph) task engines is great. I've used [Luigi](https://github.com/spotify/luigi) at work for about 2 years now. For feeding the backend of this website with data, I decided to set up an Airflow cluster.

![Airflow Screenshot](airflow.png)

As opposed to this paradigm:

- Write a script.
- Set it up in `cron`.
- Check the logs whenever something goes wrong.

You get a whole lot more bang for your buck, with similar amounts of work. DAG paradigm is more like:

- Write a script.
- Write an airflow DAG, where you call that script.
- Set a schedule for the script.
- Check on it from the built-in Web UI.
- Stop and restart tasks whenever you want to.
- View the console output from the Web UI.

It _really_ helps certain types of batch processes scale past a certain point, while simplifying the process of managing and deploying them. You can relatively easily set up Airflow to schedule thousands of tasks without all that much more configuration past what you'd normally write.

## Words of Warning

There's a number of gotchas you should know prior to using one of the DAG engines, as they all fall into the same traps.

- `cron` is _incredibly_ difficult to beat for reliability. There's a reason it's ubiquitous. DAG engines are usually for tasks that need to scale past what `cron` is built for. If you're just trying to run a single script on the regular, stick with that instead.
- Usually there's a non-trivial amount of setup involved at the beginning.
- Sometimes scaling can be a pain. For Airflow, you'll need to setup a task queue like Celery and a number of different nodes (web server, database, etc) once you scale past a certain point.
- If you're on Google Cloud Platform, they offer a hosted Airflow service called [Cloud Composer](https://cloud.google.com/composer/) which could be worth it if you want to avoid the difficulties in setting up and deploying Airflow.
- There's a [relatively mature Docker image](https://github.com/puckel/docker-airflow) that you can reuse if you don't want to configure it yourself.

## Diving In

I set up two nodes for this in my [Proxmox cluster](https://www.proxmox.com/en/):

1. **Airflow web server.** This was just based on a Ubuntu 16.04 instance.
2. **Postgres backend.** I opted to use the local scheduler, which can use Postgres as a backend, and not set up some of the larger scale options like Celery.

The LocalExecutor option (configured for using Postgres as a backend in this instance) has probably the highest payoff-to-effort ratio, compared to how difficult it can be to set up the CeleryExecutor. Once you've created the database, verified the connection works, and run `airflow initdb`, you're good to go.

Now you can start making DAGs.

## DAGS, Operators, Upstream, Huh?

There's [a lot of lingo to be learned](https://airflow.incubator.apache.org/concepts.html) when using Airflow.

- **DAG** - This is a single workflow, wherein you can arrange tasks and dependencies.
- **Operator** - This is a single unit of work that can be instantiated multiple times to achieve a particular output/goal/etc. There are things like `BashOperator` for executing shell scripts, or `PythonOperator` for python files, etc.
- **DAG Run** - A single execution of a DAG of tasks. Each operator in the DAG is instantiated and executed as it's dependencies are completed.

### Features to Avoid

Since Airflow has been in the Apache Incubator for the past year or so, it's been growing pretty rapidly with a lot of features like this. Having used it a little while now, there's a number of features that I choose to steer clear of for maintainability's sake.

- [X-coms](https://airflow.incubator.apache.org/concepts.html#xcoms). The biggest downside of Airflow compared to Luigi is that it doesn't force you to write idempotent tasks in the same way. It's easy to accidentally end up writing something that has outputs which don't reproduce. If you treat a task like a function with a single input and output, it works best.
- Branching Operators - If you need to choose between multiple sets of logic, it's best just to dynamically set your task dependencies at runtime, as opposed to creating a branching task.
- Bitshift Operators as Upstream/Downstream - Airflow recently introduced a way of setting upstream/downstream tasks by using bitshift operators like `<<` or `>>`. This is bad. Explicit is good! `set_upstream()` is not particularly verbose anyways.








---
pub_date: 2018-06-25
---
slug_title: airflow
---
twitter_handle: _zmsy
