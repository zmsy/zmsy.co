{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fantasy Baseball Pandas Analysis\n",
    "\n",
    "This takes a number of inputs and produces fantasy scores for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import datetime\n",
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "pd.options.display.max_columns = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection information for the database\n",
    "\n",
    "POSTGRES_USER = os.environ.get('POSTGRES_USER')\n",
    "POSTGRES_PASSWORD = os.environ.get('POSTGRES_PASSWORD')\n",
    "POSTGRES_IP = \"192.168.0.118\"\n",
    "POSTGRES_PORT = 5432\n",
    "POSTGRES_DB = 'postgres'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Roster Data\n",
    "\n",
    "This will rip the roster information from ESPN and save it to a local CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAGUE_URL = \"http://games.espn.com/flb/leaguerosters?leagueId={league_id}\"\n",
    "LEAGUE_ID = 15594\n",
    "\n",
    "# translate ESPN names to Fangraphs names.\n",
    "# Add new names as required, since name is used to join the data sets.\n",
    "TRANSLATIONS = {\n",
    "    'Nicky Delmonico': 'Nick Delmonico',\n",
    "    'Yuli Gurriel': 'Yulieski Gurriel'\n",
    "}\n",
    "\n",
    "\n",
    "rosters_html = requests.get(LEAGUE_URL.format(league_id=LEAGUE_ID)).text\n",
    "rosters_soup = BeautifulSoup(rosters_html, \"lxml\")\n",
    "\n",
    "rosters = rosters_soup.find_all(\"table\", {'class': 'playerTableTable'})\n",
    "\n",
    "players = []\n",
    "for roster in rosters:\n",
    "    team_name = roster.find(\"a\").text\n",
    "    players_html = roster.find_all(\"td\", {'class': 'playertablePlayerName'})\n",
    "    for player in players_html:\n",
    "        # parse player info\n",
    "        player_name = player.text.split(\",\")[0]\n",
    "        player_name = player_name.replace(\"*\", \"\")\n",
    "\n",
    "        # translate player name if necessary\n",
    "        translation = TRANSLATIONS.get(player_name)\n",
    "        if translation:\n",
    "            player_name = translation\n",
    "\n",
    "        # add to output list\n",
    "        players.append([player_name, team_name])\n",
    "\n",
    "with open(\"rosters.csv\", \"w\", newline='') as out_file:\n",
    "    writer = csv.writer(out_file)\n",
    "    writer.writerow((\"Name\", \"Squad\"))\n",
    "    writer.writerows(players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Parse Actuals\n",
    "\n",
    "Looks through the URLs to grab batting & pitching actuals and deliver those back to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static urls\n",
    "season = datetime.datetime.now().year\n",
    "PITCHERS_URL = \"https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=all&qual=0&type=c,36,37,38,40,-1,120,121,217,-1,24,41,42,43,44,-1,117,118,119,-1,6,45,124,-1,62,122,13&season={season}&month=0&season1={season}&ind=0&team=0&rost=0&age=0&filter=&players=0&page=1_100000\".format(season=season)\n",
    "BATTERS_URL = \"https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=8&season={season}&month=0&season1={season}&ind=0&team=0&rost=0&age=0&filter=&players=0&page=1_10000\".format(season=season)\n",
    "\n",
    "# # request the data\n",
    "pitchers_html = requests.get(PITCHERS_URL).text\n",
    "batters_html = requests.get(BATTERS_URL).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take the requests and parse out the relevant header information for each of the positions. This function will take one of the fangraphs pages as input and write out a CSV of that information once it's parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_array_from_fangraphs_html(input_html, out_file_name):\n",
    "    \"\"\"\n",
    "    Take a HTML stats page from fangraphs and parse it out to a CSV file.\n",
    "    \"\"\"\n",
    "    # parse input\n",
    "    soup = BeautifulSoup(input_html, \"lxml\")\n",
    "    table = soup.find(\"table\", {\"class\": \"rgMasterTable\"})\n",
    "    \n",
    "    # get headers\n",
    "    headers_html = table.find(\"thead\").find_all(\"th\")\n",
    "    headers = []\n",
    "    for header in headers_html:\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    # get rows\n",
    "    rows = []\n",
    "    rows_html = table.find(\"tbody\").find_all(\"tr\")\n",
    "    for row in rows_html:\n",
    "        row_data = []\n",
    "        for cell in row.find_all(\"td\"):\n",
    "            row_data.append(cell.text)\n",
    "        rows.append(row_data)\n",
    "    \n",
    "    # write to CSV file\n",
    "    with open(out_file_name, \"w\") as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the player data, I'm writing these out to a CSV file if I want to load them again later without having to run the requests to those pages once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_array_from_fangraphs_html(batters_html, 'batters_actuals.csv')\n",
    "parse_array_from_fangraphs_html(pitchers_html, 'pitchers_actuals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Projections\n",
    "\n",
    "For this part, we need to call some external bash code here, because the form data is too big to reasonably bring into the script here. Check out the [original blog post](https://zmsy.co/blog/fantasy-baseball/) on how to configure this for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call('./get_fangraphs.sh', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Into Pandas\n",
    "\n",
    "Load those CSV files using read_csv() in pandas. Since some of the percentage values are stored as strings, we need to parse those into floats.\n",
    "\n",
    "We want to create two dataframes here:\n",
    "\n",
    "- `dfb` - Batters Dataframe\n",
    "- `dfp` - Pitchers Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rost = pd.read_csv('rosters.csv')\n",
    "dfb_act = pd.read_csv('batters_actuals.csv')\n",
    "dfp_act = pd.read_csv('pitchers_actuals.csv')\n",
    "\n",
    "# create a function to parse out percentage strings to floats\n",
    "def parse_pctg(value):\n",
    "    return float(value.split()[0]) / 100\n",
    "\n",
    "# apply that to all percentage values in the dataframes\n",
    "dfb_act['BB%'] = dfb_act['BB%'].apply(lambda x: parse_pctg(x))\n",
    "dfb_act['K%'] = dfb_act['K%'].apply(lambda x: parse_pctg(x))\n",
    "dfp_act['K%'] = dfp_act['K%'].apply(lambda x: parse_pctg(x))\n",
    "dfp_act['BB%'] = dfp_act['BB%'].apply(lambda x: parse_pctg(x))\n",
    "dfp_act['K-BB%'] = dfp_act['K-BB%'].apply(lambda x: parse_pctg(x))\n",
    "dfp_act['LOB%'] = dfp_act['LOB%'].apply(lambda x: parse_pctg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('batters_projections.html', 'r') as bhtml:\n",
    "    btxt = bhtml.read()\n",
    "    dfb_proj = pd.read_html(btxt)[-1]  # read_html returns ALL tables, we just want the last one.\n",
    "    dfb_proj.dropna(axis=1, inplace=True)\n",
    "\n",
    "with open('pitchers_projections.html', 'r') as phtml:\n",
    "    ptxt = phtml.read()\n",
    "    dfp_proj = pd.read_html(ptxt)[-1]\n",
    "    dfp_proj.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the datasets together. we want one\n",
    "# for batters and one for pitchers, with\n",
    "# roster information in both of them.\n",
    "\n",
    "dfb = pd.merge(dfb_proj, df_rost, how='left', on='Name', suffixes=('.p', '.r'))\n",
    "dfb = pd.merge(dfb, dfb_act, how='left', on='Name', suffixes=('', '.a'))\n",
    "\n",
    "dfp = pd.merge(dfp_proj, df_rost, how='left', on='Name', suffixes=('.p', '.r'))\n",
    "dfp = pd.merge(dfp, dfp_act, how='left', on='Name', suffixes=('', '.a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and Qualify Information\n",
    "\n",
    "The dataframes for pitchers/batters contain a lot of noise for things that we don't really care about, or won't actually have much of an effect on our league."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply some filters so we can get rid of players who won't play.\n",
    "# minimum plate appearances or innings pitched\n",
    "\n",
    "dfb = dfb[dfb['PA'] > 100]\n",
    "dfp = dfp[dfp['IP'] > 20]\n",
    "\n",
    "# add in league information\n",
    "LEAGUES = {\n",
    "    'Angels': 'AL',\n",
    "    'Astros': 'AL',\n",
    "    'Athletics': 'AL',\n",
    "    'Blue Jays': 'AL',\n",
    "    'Braves': 'NL',\n",
    "    'Brewers': 'NL',\n",
    "    'Cardinals': 'NL',\n",
    "    'Cubs': 'NL',\n",
    "    'Diamondbacks': 'NL',\n",
    "    'Dodgers': 'NL',\n",
    "    'Giants': 'NL',\n",
    "    'Indians': 'AL',\n",
    "    'Mariners': 'AL',\n",
    "    'Marlins': 'NL',\n",
    "    'Mets': 'NL',\n",
    "    'Nationals': 'NL',\n",
    "    'Orioles': 'AL',\n",
    "    'Padres': 'NL',\n",
    "    'Phillies': 'NL',\n",
    "    'Pirates': 'NL',\n",
    "    'Rangers': 'AL',\n",
    "    'Rays': 'AL',\n",
    "    'Red Sox': 'AL',\n",
    "    'Reds': 'NL',\n",
    "    'Rockies': 'NL',\n",
    "    'Royals': 'AL',\n",
    "    'Tigers': 'AL',\n",
    "    'Twins': 'AL',\n",
    "    'White Sox': 'AL',\n",
    "    'Yankees': 'AL'\n",
    "}\n",
    "\n",
    "\n",
    "# derive the league for each player. my league is AL-only, so we want to focus on that.\n",
    "def league(x):\n",
    "    return LEAGUES.get(x)\n",
    "\n",
    "dfb['League'] = dfb['Team'].apply(lambda x: league(x))\n",
    "dfp['League'] = dfp['Team'].apply(lambda x: league(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange columns for readability and filter some out\n",
    "# keep only ones relevant for our league\n",
    "dfb_columns = ['#', 'Name', 'Squad', 'Team', 'League', 'PA', 'PA.a', 'AB', 'H', 'SO', 'K%', 'HR', 'HR.a', 'AVG',\n",
    "     'ISO', 'BABIP', 'wRC+', 'AVG.a', 'OBP', 'OBP.a', 'wOBA', 'wOBA.a', 'SLG', 'SLG.a', 'OPS', 'BB%', 'BB']\n",
    "\n",
    "dfp_columns = ['#', 'Name', 'Squad', 'Team', 'League', 'IP', 'ERA', 'ER', 'HR', 'SO', 'BB', 'WHIP', 'K/9', 'BB/9', 'FIP',\n",
    "     'K/9.a', 'BB/9.a', 'K/BB', 'K%', 'WHIP.a', 'SO.a', 'LOB%', 'ERA.a', 'FIP.a', 'E-F', 'xFIP', 'SIERA', 'IP.a']\n",
    "\n",
    "\n",
    "dfb = dfb[dfb_columns]\n",
    "dfp = dfp[dfp_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Scores\n",
    "\n",
    "The individual players in both the batting and pitching groups will get scored based on the entirety of the sample available. We calculate a composite score by taking the individual z-scores in each of the categories and trying to determine which players are above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 1 represents a positive number, i.e. higher is better\n",
    "# a -1 represents negative, meaning lower is better\n",
    "\n",
    "dfb_score_cols = {\n",
    "    'PA': 1,\n",
    "    'K%': -1,\n",
    "    'HR': 1,\n",
    "    'ISO': 1,\n",
    "    'OBP': 1,\n",
    "    'wOBA': 1,\n",
    "    'SLG': 1\n",
    "}\n",
    "\n",
    "dfp_score_cols = {\n",
    "    'IP': 1,\n",
    "    'ERA': -1,\n",
    "    'HR': -1,\n",
    "    'SO': 1,\n",
    "    'WHIP': -1,\n",
    "    'FIP': -1,\n",
    "    'K/9': 1\n",
    "}\n",
    "\n",
    "for col in dfb_score_cols.keys():\n",
    "    col_score = col + \"_score\"\n",
    "    dfb[col_score] = (dfb[col] - dfb[col].mean()) / dfb[col].std(ddof=0) * dfb_score_cols.get(col)\n",
    "    \n",
    "\n",
    "for col in dfp_score_cols.keys():\n",
    "    col_score = col + \"_score\"\n",
    "    dfp[col_score] = (dfp[col] - dfp[col].mean()) / dfp[col].std(ddof=0) * dfp_score_cols.get(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Information Back to Database\n",
    "\n",
    "Once the table is available in the database, then we can query it again using other tools to make our lives easier. Then it can be used to display the information about each player in the Superset DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlalchemy.create_engine(\"postgres://{user}:{password}@{host}:{port}/{db}\".format(\n",
    "    user=POSTGRES_USER,\n",
    "    password=POSTGRES_PASSWORD,\n",
    "    host=POSTGRES_IP,\n",
    "    port=POSTGRES_PORT,\n",
    "    db=POSTGRES_DB\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x7f2789956d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute 'cursor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a37f180d0efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Engine' object has no attribute 'cursor'"
     ]
    }
   ],
   "source": [
    "conn.cursor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
