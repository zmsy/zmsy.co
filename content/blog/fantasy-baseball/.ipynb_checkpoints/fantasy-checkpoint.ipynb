{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fantasy Baseball Pandas Analysis\n",
    "\n",
    "This takes a number of inputs and produces fantasy scores for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import datetime\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Roster Data\n",
    "\n",
    "This will rip the roster information from ESPN and save it to a local CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAGUE_URL = \"http://games.espn.com/flb/leaguerosters?leagueId={league_id}\"\n",
    "LEAGUE_ID = 15594\n",
    "\n",
    "# translate ESPN names to Fangraphs names.\n",
    "# Add new names as required, since name is used to join the data sets.\n",
    "TRANSLATIONS = {\n",
    "    'Nicky Delmonico': 'Nick Delmonico',\n",
    "    'Yuli Gurriel': 'Yulieski Gurriel'\n",
    "}\n",
    "\n",
    "\n",
    "rosters_html = requests.get(LEAGUE_URL.format(league_id=LEAGUE_ID)).text\n",
    "rosters_soup = BeautifulSoup(rosters_html, \"lxml\")\n",
    "\n",
    "rosters = rosters_soup.find_all(\"table\", {'class': 'playerTableTable'})\n",
    "\n",
    "players = []\n",
    "for roster in rosters:\n",
    "    team_name = roster.find(\"a\").text\n",
    "    players_html = roster.find_all(\"td\", {'class': 'playertablePlayerName'})\n",
    "    for player in players_html:\n",
    "        # parse player info\n",
    "        player_name = player.text.split(\",\")[0]\n",
    "        player_name = player_name.replace(\"*\", \"\")\n",
    "\n",
    "        # translate player name if necessary\n",
    "        translation = TRANSLATIONS.get(player_name)\n",
    "        if translation:\n",
    "            player_name = translation\n",
    "\n",
    "        # add to output list\n",
    "        players.append([player_name, team_name])\n",
    "\n",
    "with open(\"rosters.csv\", \"w\", newline='') as out_file:\n",
    "    writer = csv.writer(out_file)\n",
    "    writer.writerow((\"Name\", \"Squad\"))\n",
    "    writer.writerows(players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Parse Actuals\n",
    "\n",
    "Looks through the URLs to grab batting & pitching actuals and deliver those back to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static urls\n",
    "season = datetime.datetime.now().year\n",
    "PITCHERS_URL = \"https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=all&qual=0&type=c,36,37,38,40,-1,120,121,217,-1,24,41,42,43,44,-1,117,118,119,-1,6,45,124,-1,62,122,13&season={season}&month=0&season1={season}&ind=0&team=0&rost=0&age=0&filter=&players=0&page=1_100000\".format(season=season)\n",
    "BATTERS_URL = \"https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=8&season={season}&month=0&season1={season}&ind=0&team=0&rost=0&age=0&filter=&players=0&page=1_10000\".format(season=season)\n",
    "\n",
    "# # request the data\n",
    "pitchers_html = requests.get(PITCHERS_URL).text\n",
    "batters_html = requests.get(BATTERS_URL).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take the requests and parse out the relevant header information for each of the positions. This function will take one of the fangraphs pages as input and write out a CSV of that information once it's parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_array_from_fangraphs_html(input_html, out_file_name):\n",
    "    \"\"\"\n",
    "    Take a HTML stats page from fangraphs and parse it out to a CSV file.\n",
    "    \"\"\"\n",
    "    # parse input\n",
    "    soup = BeautifulSoup(input_html, \"lxml\")\n",
    "    table = soup.find(\"table\", {\"class\": \"rgMasterTable\"})\n",
    "    \n",
    "    # get headers\n",
    "    headers_html = table.find(\"thead\").find_all(\"th\")\n",
    "    headers = []\n",
    "    for header in headers_html:\n",
    "        headers.append(header.text)\n",
    "    \n",
    "    # get rows\n",
    "    rows = []\n",
    "    rows_html = table.find(\"tbody\").find_all(\"tr\")\n",
    "    for row in rows_html:\n",
    "        row_data = []\n",
    "        for cell in row.find_all(\"td\"):\n",
    "            row_data.append(cell.text)\n",
    "        rows.append(row_data)\n",
    "    \n",
    "    # write to CSV file\n",
    "    with open(out_file_name, \"w\") as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the player data, I'm writing these out to a CSV file if I want to load them again later without having to run the requests to those pages once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Name', 'Team', 'G', 'PA', 'HR', 'R', 'RBI', 'SB', 'BB%', 'K%', 'ISO', 'BABIP', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'BsR', 'Off', 'Def', 'WAR']\n",
      "['#', 'Name', 'Team', 'K/9', 'BB/9', 'K/BB', 'HR/9', 'K%', 'BB%', 'K-BB%', 'SO', 'AVG', 'WHIP', 'BABIP', 'LOB%', 'ERA-', 'FIP-', 'xFIP-', 'ERA', 'FIP', 'E-F', 'xFIP', 'SIERA', 'IP']\n"
     ]
    }
   ],
   "source": [
    "parse_array_from_fangraphs_html(batters_html, 'batters_actuals.csv')\n",
    "parse_array_from_fangraphs_html(pitchers_html, 'pitchers_actuals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Projections\n",
    "\n",
    "For this part, we need to call some external bash code here, because the form data is too big to reasonably bring into the script here. Check out the [original blog post](https://zmsy.co/blog/fantasy-baseball/) on how to configure this for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call('./get_fangraphs.sh', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Into Pandas\n",
    "\n",
    "Load those CSV files using read_csv() in pandas. Since some of the percentage values are stored as strings, we need to parse those into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rost = pd.read_csv('rosters.csv')\n",
    "dfb_act = pd.read_csv('batters_actuals.csv')\n",
    "dfp_act = pd.read_csv('pitchers_actuals.csv')\n",
    "\n",
    "# create a function to parse out percentage strings to floats\n",
    "def parse_pctg(value):\n",
    "    return float(value.split()[0]) / 100\n",
    "\n",
    "# apply that to all percentage values in the dataframes\n",
    "dfb_act['BB%'] = dfb_act['BB%'].apply(lambda x: parse_pctg(x))\n",
    "dfb_act['K%'] = dfb_act['K%'].apply(lambda x: parse_pctg(x))\n",
    "dfp_act['K%'] = dfp_act['K%'].apply(lambda x: parse_pctg(x))\n",
    "dfp_act['BB%'] = dfp_act['BB%'].apply(lambda x: parse_pctg(x))\n",
    "dfp_act['K-BB%'] = dfp_act['K-BB%'].apply(lambda x: parse_pctg(x))\n",
    "dfp_act['LOB%'] = dfp_act['LOB%'].apply(lambda x: parse_pctg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('batters_projections.html', 'r') as bhtml:\n",
    "    btxt = bhtml.read()\n",
    "    dfb_proj = pd.read_html(btxt)[-1]  # read_html returns ALL tables, we just want the last one.\n",
    "    dfb_proj.dropna(axis=1, inplace=True)\n",
    "\n",
    "with open('pitchers_projections.html', 'r') as phtml:\n",
    "    ptxt = phtml.read()\n",
    "    dfp_proj = pd.read_html(ptxt)[-1]\n",
    "    dfp_proj.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the datasets together. we want one\n",
    "# for batters and one for pitchers, with\n",
    "# roster information in both of them.\n",
    "\n",
    "dfb = pd.merge(dfb_proj, df_rost, how='left', on='Name', suffixes=('.p', '.r'))\n",
    "dfb = pd.merge(dfb, dfb_act, how='left', on='Name', suffixes=('', '.a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.fangraphs.com/leaders.aspx?pos=all&stats=pit&lg=al&qual=0&type=c,36,37,38,40,-1,120,121,217,-1,24,41,42,43,44,-1,117,118,119,-1,6,45,124,-1,62,122,13&season=2018&month=0&season1=2018&ind=0&team=0&rost=0&age=0&filter=&players=0&page=1_100000'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PITCHERS_URL.format(season=season)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
